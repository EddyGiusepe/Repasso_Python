{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "salvando_TENSORFLOW.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOakouktwy/mqhj2WNBg+hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EddyGiusepe/Repasso_Python/blob/main/salvando_TENSORFLOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <h2 align=\"center\">Usando Google Colab</h2>\n",
        "\n",
        "\n",
        "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro\n",
        "\n",
        "\n",
        "Este Script foi baseado no [Machine Learning Mastery - Jason Brownlee PhD](https://machinelearningmastery.com/google-colab-for-machine-learning-projects/)"
      ],
      "metadata": {
        "id": "fCH9_do8fCr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo: salvar o progresso do modelo no Google Drive"
      ],
      "metadata": {
        "id": "V4ytMAH_fbkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O ``Google Colab`` é provavelmente a maneira mais fácil de nos fornecer recursos avançados de ``GPU`` para seu projeto de aprendizado de máquina. Mas na versão gratuita do Colab, o Google limita o tempo que podemos usar nosso notebook Colab em cada sessão. Nosso kernel pode terminar sem motivo. Podemos reiniciar nosso notebook e continuar nosso trabalho, mas podemos perder tudo na memória. Isso é um problema se precisarmos treinar nosso modelo por muito tempo. Nossa instância do Colab pode ser encerrada antes que o treinamento seja concluído.\n",
        "\n",
        "Usando a extensão do Google Colab para montar nosso retorno de chamada do ``Google Drive`` e ``Keras ModelCheckpoint``, podemos salvar o progresso do nosso modelo no Google Drive. Isso é particularmente útil para contornar os tempos limite do Colab. É mais tolerante para usuários pagos do ``Pro`` e ``Pro+``, mas sempre há uma chance de que nosso treinamento de modelo termine no meio do caminho em momentos aleatórios. É valioso se não quisermos perder nosso modelo parcialmente treinado.\n",
        "\n",
        "Para esta demonstração, usaremos o modelo ``LeNet-5`` no conjunto de dados ``MNIST``."
      ],
      "metadata": {
        "id": "xlahGUstfwv2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zu6CS9_je_kZ"
      },
      "outputs": [],
      "source": [
        "# Importamos algumas bibliotecas necessárias\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Conv2D, Flatten, MaxPool2D\n",
        "from keras.models import Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma classe\n",
        "\n",
        "class LeNet5(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LeNet5, self).__init__()\n",
        "    \n",
        "    #creating layers in initializer\n",
        "    self.conv1 = Conv2D(filters=6, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n",
        "    self.max_pool2x2 = MaxPool2D(pool_size=(2,2))\n",
        "    self.conv2 = Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", activation=\"relu\")\n",
        "    self.flatten = Flatten()\n",
        "    self.fc1 = Dense(units=120, activation=\"relu\")\n",
        "    self.fc2 = Dense(units=84, activation=\"relu\")\n",
        "    self.fc3=Dense(units=10, activation=\"softmax\")\n",
        "  \n",
        "  def call(self, input_tensor):\n",
        "    conv1 = self.conv1(input_tensor)\n",
        "    maxpool1 = self.max_pool2x2(conv1)\n",
        "    conv2 = self.conv2(maxpool1)\n",
        "    maxpool2 = self.max_pool2x2(conv2)\n",
        "    flatten = self.flatten(maxpool2)\n",
        "    fc1 = self.fc1(flatten)\n",
        "    fc2 = self.fc2(fc1)\n",
        "    fc3 = self.fc3(fc2)\n",
        "    return fc3"
      ],
      "metadata": {
        "id": "un9r2TuVfQjV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, para ``salvar o progresso do modelo durante o treinamento no Google Drive``, primeiro precisamos montar nosso Google Drive em nosso ambiente Colab."
      ],
      "metadata": {
        "id": "Kpawp0crhPF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciamos nosso Google Drive\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "MOUNTPOINT = \"/content/gdrive\"\n",
        "DATADIR = os.path.join(MOUNTPOINT, \"MyDrive\")\n",
        "drive.mount(MOUNTPOINT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7tlcG9qhKMk",
        "outputId": "13eb77d4-4b9c-4f21-cb04-08b10a754d92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois, declaramos o Callback para salvar nosso modelo de checkpoint no Google Drive."
      ],
      "metadata": {
        "id": "Zyu-EMu-hiTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Observarás que será criada uma pasta com o nome: checkpoints\n",
        " \n",
        "checkpoint_path = DATADIR + \"/checkpoints/cp-epoch-{epoch}.ckpt\"\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "htVJPIGMhcKa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, começamos o treinamento no conjunto de dados ``MNIST`` com os retornos de chamada do ponto de verificação para garantir que possamos retomar na última época, caso a sessão do Colab expire:"
      ],
      "metadata": {
        "id": "fwdflzX4h432"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixamos os Dados\n",
        "mnist_digits = keras.datasets.mnist\n",
        "\n",
        "# Separamos em Dados de Treinamento e Dados de teste\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist_digits.load_data()\n",
        " \n",
        "input_layer = Input(shape=(28,28,1))\n",
        "\n",
        "# Instanciamos nosso Modelo\n",
        "model = LeNet5()(input_layer)\n",
        "model = Model(inputs=input_layer, outputs=model)\n",
        "\n",
        "# Compilamos nosso Modelo\n",
        "model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=\"acc\")\n",
        "\n",
        "# Fit\n",
        "model.fit(x=train_images, y=train_labels, batch_size=256,\n",
        "          validation_data = [test_images, test_labels], epochs=5, callbacks=[cp_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av3gPqYOhyQg",
        "outputId": "3afe5a0e-43b0-4b3c-fec4-5f0863cd769d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 1.3052 - acc: 0.8325\n",
            "Epoch 1: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-1.ckpt\n",
            "235/235 [==============================] - 42s 174ms/step - loss: 1.3052 - acc: 0.8325 - val_loss: 0.1932 - val_acc: 0.9456\n",
            "Epoch 2/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1503 - acc: 0.9563\n",
            "Epoch 2: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-2.ckpt\n",
            "235/235 [==============================] - 41s 173ms/step - loss: 0.1503 - acc: 0.9563 - val_loss: 0.1059 - val_acc: 0.9684\n",
            "Epoch 3/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0909 - acc: 0.9725\n",
            "Epoch 3: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-3.ckpt\n",
            "235/235 [==============================] - 41s 173ms/step - loss: 0.0909 - acc: 0.9725 - val_loss: 0.0849 - val_acc: 0.9756\n",
            "Epoch 4/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0643 - acc: 0.9799\n",
            "Epoch 4: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-4.ckpt\n",
            "235/235 [==============================] - 41s 174ms/step - loss: 0.0643 - acc: 0.9799 - val_loss: 0.0732 - val_acc: 0.9779\n",
            "Epoch 5/5\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0504 - acc: 0.9842\n",
            "Epoch 5: saving model to /content/gdrive/MyDrive/checkpoints/cp-epoch-5.ckpt\n",
            "235/235 [==============================] - 41s 173ms/step - loss: 0.0504 - acc: 0.9842 - val_loss: 0.0662 - val_acc: 0.9806\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc9f6c09e90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E a partir da saída, podemos ver que os pontos de verificação foram salvos. Olhando para minha pasta do Google Drive, também podemos ver os pontos de verificação armazenados lá.\n",
        "\n",
        "\n",
        "A instância do Colab está no ambiente de nuvem do Google. A máquina que está rodando tem algum armazenamento, então podemos instalar um pacote ou baixar alguns arquivos nele. No entanto, não devemos salvar nosso ponto de verificação lá porque não temos garantia de que o recuperaremos depois que nossa sessão for encerrada. Portanto, acima, montamos nosso Google Drive na instância e salvamos o checkpoint em nosso Google Drive. É assim que podemos ter certeza de que os arquivos do ponto de verificação estão acessíveis."
      ],
      "metadata": {
        "id": "l7UK2w9XjvCH"
      }
    }
  ]
}