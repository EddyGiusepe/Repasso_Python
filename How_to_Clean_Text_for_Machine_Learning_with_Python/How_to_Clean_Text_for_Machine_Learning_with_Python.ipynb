{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">How to Clean Text for Machine Learning with Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link de estudo:\n",
    "\n",
    "* [Clean text for ML with Python](https://machinelearningmastery.com/clean-text-machine-learning-python/)\n",
    "\n",
    "* [Guia definitivo para lidar com dados de texto (usando Python) – para cientistas e engenheiros de dados](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É bem sabido que você não pode ir direto do texto bruto para o ajuste de um modelo de `aprendizado de máquina` ou `aprendizado profundo`.\n",
    "\n",
    "Você deve limpar seu texto primeiro, o que significa dividi-lo em palavras e lidar com pontuação e maiúsculas e minúsculas.\n",
    "\n",
    "Na verdade, há todo um conjunto de métodos de preparação de texto que você pode precisar usar, e a escolha dos métodos realmente depende de sua **tarefa** de processamento de linguagem natural.\n",
    "\n",
    "Aqui aprenderemos a como limpar e preparar nosso texto pronto para modelagem com aprendizado de máquina. Os pontos a discutir são:\n",
    "\n",
    "* como começar desenvolvendo suas próprias ferramentas simples de limpeza de texto.\n",
    "\n",
    "* como dar um passo à frente e usar os \"métodos mais sofisticados\" da biblioteca `NLTK` (Aqui recomendamos, ver também, o Hugging face ... o último em tecnologia para NLP).\n",
    "\n",
    "* como preparar texto ao usar **métodos modernos** de representação de texto, como `Word Embeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metamorfose de Franz Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar selecionando um conjunto de dados.\n",
    "\n",
    "Neste tutorial, usaremos o texto do livro `Metamorfose de Franz Kafka`. Nenhuma razão específica, além de ser curto, eu gosto, e você pode gostar também. Espero que seja um daqueles clássicos que a maioria dos alunos tem que ler na escola.\n",
    "\n",
    "O texto completo de Metamorphosis está disponível gratuitamente no Project Gutenberg.\n",
    "\n",
    "* [Metamorfose de Franz Kafka sobre o Projeto Gutenberg](https://www.gutenberg.org/ebooks/5200).\n",
    "\n",
    "Você pode baixar a versão em texto ASCII do texto aqui:\n",
    "\n",
    "* [Metamorfose por Franz Kafka Texto simples UTF-8](https://www.gutenberg.org/cache/epub/5200/pg5200.txt) (pode ser necessário carregar a página duas vezes).\n",
    "\n",
    "Baixe o arquivo e coloque-o em seu diretório de trabalho atual com o nome de arquivo `metamorphosis.txt`.\n",
    "\n",
    "O arquivo contém informações de cabeçalho e rodapé nas quais não estamos interessados, especificamente informações sobre direitos autorais e licença. Abra o arquivo e exclua as informações de cabeçalho e rodapé e salve o arquivo como `metamorphosis_clean.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Limpeza de Texto é Específica da Tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de realmente obter seus dados de texto, o primeiro passo para limpar os dados de texto é ter uma ideia forte sobre o que você está tentando alcançar e, nesse contexto, revisar seu texto para ver o que exatamente pode ajudar.\n",
    "\n",
    "Tome um momento para olhar para o texto. `O que você percebe?`\n",
    "\n",
    "Aqui está o que eu vejo:\n",
    "\n",
    "\n",
    "* É um texto simples, então não há marcação para analisar (yay!).\n",
    "\n",
    "* A tradução do original alemão usa o inglês do Reino Unido (UK) (por exemplo, \"travelling\").\n",
    "\n",
    "* As linhas são quebradas artificialmente com novas linhas em cerca de 70 caracteres (meh).\n",
    "\n",
    "* Não há erros de digitação ou ortografia óbvios.\n",
    "\n",
    "* Há pontuação como vírgulas, apóstrofos, aspas, pontos de interrogação e muito mais.\n",
    "\n",
    "* Há descrições hifenizadas como “armour-like”.\n",
    "\n",
    "* Há muito uso do traço (\"-\") para continuar as frases (talvez substituir por vírgulas?).\n",
    "\n",
    "* Existem nomes (por exemplo: \"Mr. Samsa\")\n",
    "\n",
    "* Não parece haver números que requeiram manipulação (por exemplo: 1999)\n",
    "\n",
    "* Existem marcadores de seção (por exemplo, \"II\" e \"III\") e removemos o primeiro \"I\".\n",
    "\n",
    "\n",
    "\n",
    "<font color=\"orange\">Tenho certeza de que há muitas coisas mais acontecendo para o olho treinado.</font>\n",
    "\n",
    "\n",
    "No entanto, considere alguns possíveis objetivos que podemos ter ao trabalhar com este documento de texto.\n",
    "\n",
    "Por exemplo:\n",
    "\n",
    "* Se estivéssemos interessados ​​em desenvolver um modelo de `linguagem Kafkiana`, podemos querer manter todas as maiúsculas e minúsculas, aspas e outras pontuações no lugar.\n",
    "\n",
    "* Se estivéssemos interessados ​​em classificar documentos como \"Kafka\" e \"Não Kafka\", talvez devêssemos eliminar maiúsculas e minúsculas, pontuação e até mesmo **cortar as palavras de volta ao seu radical**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">A limpeza de texto é difícil</font>, mas o texto com o qual escolhemos trabalhar já está bem limpo.\n",
    "\n",
    "Poderíamos apenas escrever algum código `Python` para limpá-lo manualmente, e este é um bom exercício para aqueles problemas simples que você encontrar. Ferramentas como expressões regulares e divisão de strings podem te ajudar bastante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregar Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos carregar os dados de texto para que possamos trabalhar com eles.\n",
    "\n",
    "O texto é pequeno e será carregado rapidamente e caberá facilmente na memória. Isso nem sempre será o caso e você pode precisar escrever código para mapear a memória do arquivo. Ferramentas como `NLTK` (abordadas na próxima seção) facilitarão muito o trabalho com arquivos grandes.\n",
    "\n",
    "Podemos carregar todo o “ metamorphosis_clean.txt ” na memória da seguinte forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "#print(text)\n",
    "# A execução do exemplo carrega o arquivo inteiro na memória, pronto para trabalhar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividir por espaço em branco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto limpo geralmente significa uma lista de palavras ou tokens com os quais podemos trabalhar em nossos modelos de aprendizado de máquina.\n",
    "\n",
    "Isso significa converter o **texto bruto** em uma **lista de palavras** e salvá-lo novamente.\n",
    "\n",
    "Uma maneira muito simples de fazer isso seria dividir o documento por espaço em branco, incluindo \" \", novas linhas, tabulações e muito mais. Podemos fazer isso em Python com a função `split()` na string carregada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffOne', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A execução do exemplo divide o documento em uma longa lista de palavras e imprime as primeiras 100 para revisão.\n",
    "\n",
    "Podemos ver que a pontuação é preservada (<font color=\"yellow\">por exemplo:</font> \"wasn't\" e \"armour-like\"), o que é bom. Também podemos ver que a pontuação do final da frase é mantida com a última palavra (<font color=\"yellow\">por exemplo:</font> \"thought.\"), o que não é ótimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecione palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra abordagem pode ser usar o modelo regex (`re`) e dividir (split) o documento em palavras selecionando strings de caracteres alfanuméricos (`a-z`, `A-Z`, `0-9` e '`_`').\n",
    "\n",
    "Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split based on words only\n",
    "import re\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, executando o exemplo, podemos ver que obtemos nossa lista de palavras. Desta vez, podemos ver que <font color=\"orange\">\"armour-like\"</font> agora são duas palavras \"armour\" e \"like\" (fine), mas contrações como <font color=\"orange\">\"What's\"</font> também são duas palavras \"What\" e \"s\" (não é ótimo não)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dividir por espaço em branco e remover pontuação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos querer as palavras, mas sem a pontuação como vírgulas e aspas. Também queremos manter as contrações juntas.\n",
    "\n",
    "Uma maneira seria dividir o documento em palavras por espaço em branco (como em \"2. Dividir por espaço em branco\") e, em seguida, usar a tradução de strings para substituir toda a pontuação por nada (por exemplo, removê-la).\n",
    "\n",
    "Python fornece uma constante chamada `string.punctuation` que fornece uma grande lista de caracteres de pontuação. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Python oferece uma função chamada [translate()](https://docs.python.org/3/library/stdtypes.html#str.translate) que mapeará um conjunto de caracteres para outro.\n",
    "\n",
    "Podemos usar a função [maketrans()](https://docs.python.org/3/library/stdtypes.html#str.maketrans) para criar uma tabela de mapeamento. Podemos criar uma tabela de mapeamento vazia, mas o terceiro argumento dessa função nos permite listar todos os caracteres a serem removidos durante o processo de tradução. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos juntar tudo isso, carregar o arquivo de texto, dividi-lo em palavras por espaço em branco e traduzir cada palavra para remover a pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffOne', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'Whats', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasnt', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que isso teve o efeito desejado, principalmente.\n",
    "\n",
    "Contrações como <font color=\"orange\">\"What's\"</font> tornaram-se <font color=\"orange\">\"Whats\"</font>, mas <font color=\"orange\">\"armour-like\"</font> tornou-se <font color=\"orange\">\"armourlike\"</font>.\n",
    "\n",
    "Se você sabe alguma coisa sobre `regex`, sabe que as coisas podem ficar complexas a partir daqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalizando Caso (Normalizing Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É comum converter todas as palavras em um caso (case). Isso significa que o vocabulário diminuirá de tamanho, mas algumas distinções serão perdidas (por exemplo: <font color=\"orange\">\"Apple\"</font> a empresa versus <font color=\"orange\">\"apple\"</font> a fruta é um exemplo comumente usado).\n",
    "\n",
    "Podemos converter todas as palavras para letras minúsculas chamando a função `lower()` em cada palavra.\n",
    "\n",
    "Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffone', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"what\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'it', \"wasn't\", 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])\n",
    "\n",
    "\n",
    "# Executando esta célula, podemos ver que todas as palavras agora estão em letras minúsculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Observação:</font>\n",
    "\n",
    "A limpeza do texto é realmente difícil, específica do problema e cheia de compensações.\n",
    "\n",
    "Lembre-se, <font color=\"orange\">simples é melhor</font>.\n",
    "\n",
    "Dados de texto mais simples, modelos mais simples, vocabulários menores. Você sempre pode tornar as coisas mais complexas depois para ver se isso resulta em uma melhor habilidade de modelo.\n",
    "\n",
    "A seguir, veremos algumas das ferramentas da `biblioteca NLTK` que oferecem mais do que simples divisão de strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização e limpeza com NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O [Natural Language Toolkit](https://www.nltk.org/) , ou `NLTK` para abreviar, é uma biblioteca Python escrita para trabalhar e modelar texto.\n",
    "\n",
    "Ele fornece boas ferramentas para carregar e limpar texto que podemos usar para preparar nossos dados para trabalhar com aprendizado de máquina e algoritmos de aprendizado profundo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instale o NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/eddygiusepe/Documentos/Repasso_Python/venv_repasso/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /home/eddygiusepe/Documentos/Repasso_Python/venv_repasso/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /home/eddygiusepe/Documentos/Repasso_Python/venv_repasso/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/eddygiusepe/Documentos/Repasso_Python/venv_repasso/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /home/eddygiusepe/Documentos/Repasso_Python/venv_repasso/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/eddygiusepe/Documentos/Repasso_Python/venv_repasso/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Você pode instalar o NLTK usando seu gerenciador de pacotes favorito, como o pip:\n",
    "\n",
    "%pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a instalação, você precisará instalar os dados usados ​​com a biblioteca, incluindo um grande conjunto de documentos que você pode usar posteriormente para testar outras ferramentas no NLTK.\n",
    "\n",
    "Existem algumas maneiras de fazer isso, como de dentro de um script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou na linha de comando:\n",
    "\n",
    "```\n",
    "python -m nltk.downloader all\n",
    "```\n",
    "\n",
    "ver também os seguintes links, para Instalar e Configurar o NLTK:\n",
    "\n",
    "\n",
    "* [Instalando o NLTK](https://www.nltk.org/install.html)\n",
    "\n",
    "* [Instalando Dados NLTK](https://www.nltk.org/data.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividido em Sentenças"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um bom primeiro passo útil é dividir o texto em frases.\n",
    "\n",
    "Algumas tarefas de modelagem preferem que a entrada esteja na forma de **parágrafos ou sentenças**, como `word2vec`. Você pode primeiro dividir seu texto em frases, dividir cada frase em palavras e salvar cada frase em um arquivo, uma por linha.\n",
    "\n",
    "O `NLTK` fornece a função `sent_tokenize()` para dividir o texto em sentenças.\n",
    "\n",
    "O exemplo abaixo carrega o arquivo `metamorphosis_clean.txt` na memória, divide-o em sentenças e imprime a primeira sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into sentences\n",
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando o exemplo, podemos ver que, embora o documento seja dividido em sentenças, cada sentença ainda preserva a nova linha da quebra artificial das linhas no documento original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dividir em palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `NLTK` fornece uma função chamada `word_tokenize()` para dividir **strings** em **tokens** (nominalmente palavras).\n",
    "\n",
    "Ele divide tokens com base em espaço em branco e pontuação. <font color=\"yellow\">Por exemplo:</font> vírgulas e pontos são considerados tokens separados. As contrações são divididas (<font color=\"yellow\">por exemplo:</font> \" `What's` \" become \" `What` \" \" `'s` \"). As cotações são mantidas, e assim por diante.\n",
    "\n",
    "Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffOne', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando o código, podemos ver que a pontuação agora são `tokens` que poderíamos decidir filtrar especificamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtre a pontuação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos filtrar todos os tokens nos quais não estamos interessados, **como todas as pontuações independentes**.\n",
    "\n",
    "Isso pode ser feito iterando todos os tokens e mantendo apenas os tokens alfabéticos. `Python` tem a função `isalpha()` que pode ser usada. \n",
    "\n",
    "Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando o exemplo, você pode ver que não apenas tokens de pontuação, mas exemplos como \"`armour-like`\" e \"`'s`\" também foram filtrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filtre Stop Words (e Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stop words` são aquelas palavras que não contribuem para o significado mais profundo da frase. São as palavras mais comuns como: \"the\", \"a\", e \"is\".\n",
    "\n",
    "Para algumas aplicações, como `classificação de documentação`, pode fazer sentido remover `stop words`.\n",
    "\n",
    "O `NLTK` fornece uma lista de stop words comumente aceitas para uma variedade de idiomas, como o `inglês`. Eles podem ser carregados da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('venv_repasso': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbbafe9b4d27f1a54c71cc0b076f18a0914c7fba812b9ed6e564e8656f6468de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
